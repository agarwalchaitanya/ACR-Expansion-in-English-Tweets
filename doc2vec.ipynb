{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from acrlist import acr\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tweets = []\n",
    "train_data_acronyms = []\n",
    "acr_count = 0\n",
    "\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)) as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        train_data_tweets.append(tweet.split())\n",
    "        train_data_acronyms.append(expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['@vinijrmadrid', 'yh', 'i', 'agree', 'frenkie', 'is', 'overrated.', 'however,', 'have', 'you', 'ever', 'seen', 'a', 'gg', 'from', 'varane', 'without', 'ramos', 'by', 'his', 'side?'], tags=['good game'])]\n"
     ]
    }
   ],
   "source": [
    "def create_tagged_document(split_tweets):\n",
    "  for i, tweet in enumerate(split_tweets):\n",
    "    yield gensim.models.doc2vec.TaggedDocument(tweet, [train_data_acronyms[i]])\n",
    "    \n",
    "train_data = list(create_tagged_document(train_data_tweets))\n",
    "print(train_data[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_soze=50, min_count=2, epochs=100)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Vector & Prediction:\n",
      "[array([ 3.30729224e-02,  4.25219387e-02,  2.14383215e-01,  2.31502920e-01,\n",
      "        8.92790109e-02,  2.60980248e-01,  1.90944389e-01, -2.30320748e-02,\n",
      "        2.91241258e-01,  2.43776634e-01,  1.51321203e-01, -2.26441965e-01,\n",
      "        5.35409153e-01, -3.85960609e-01,  5.32175064e-01,  8.91531855e-02,\n",
      "        7.58388713e-02, -1.80071354e-01,  4.46996033e-01, -1.73472285e-01,\n",
      "        4.09612507e-01,  1.26741797e-01,  6.08486354e-01, -2.49143824e-01,\n",
      "       -5.57823300e-01, -1.99070483e-01,  5.10574803e-02, -3.76354828e-02,\n",
      "        3.28518569e-01, -2.47748252e-02, -4.11353230e-01, -6.31013885e-03,\n",
      "        1.05936393e-01,  2.90370416e-02, -2.32851058e-01,  2.32075751e-01,\n",
      "       -2.14494064e-01,  2.30996713e-01,  2.46923212e-02, -1.59332559e-01,\n",
      "       -6.52951375e-02,  4.74129207e-02,  2.25286677e-01, -1.93009842e-02,\n",
      "        1.13294996e-01,  7.19480872e-01,  2.74504662e-01, -4.52864707e-01,\n",
      "       -1.41044939e-02,  3.13733280e-01, -3.86825860e-01, -1.67045638e-01,\n",
      "        5.83223581e-01,  7.79475644e-02, -2.48088032e-01,  1.94301784e-01,\n",
      "        6.95707679e-01, -1.76638439e-01, -1.04361422e-01,  3.10002714e-01,\n",
      "        2.40690574e-01, -3.41376960e-02, -1.01178020e-01,  3.63794714e-01,\n",
      "        1.32061401e-02,  6.25889122e-01,  6.11770213e-01,  2.81789273e-01,\n",
      "        3.36255074e-01,  1.47580028e-01,  3.80557269e-01, -1.91942930e-01,\n",
      "        1.09974414e-01, -2.08199695e-02, -2.26704776e-01,  1.46812588e-01,\n",
      "        3.65834646e-02,  1.36678712e-03,  7.27275684e-02,  3.03496510e-01,\n",
      "       -8.86151493e-02,  2.24047676e-01, -1.70816496e-01,  4.19584662e-01,\n",
      "       -6.06218457e-01, -3.49008828e-01, -4.25759465e-01,  3.61081213e-02,\n",
      "       -5.25441647e-01,  5.20132124e-01, -2.02945665e-01,  1.21000275e-01,\n",
      "        3.82407486e-01, -6.41633233e-04, -2.41160527e-01,  2.07234651e-01,\n",
      "        2.63027370e-01,  1.17453612e-01,  1.16741188e-01,  1.86441302e-01],\n",
      "      dtype=float32)]\n",
      "('good game', 0.5888423323631287)\n"
     ]
    }
   ],
   "source": [
    "v = model.infer_vector(\"el classico was gg\".split())\n",
    "res_tup = model.docvecs.most_similar([v])[0]\n",
    "print(\"Doc2Vec Vector & Prediction:\")\n",
    "print([v])\n",
    "print(res_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, Y_train, X_test, Y_test):\n",
    "  #takes Doc2Vec as input layer instead of Word Embeddings, and trains classifiers for each acronym\n",
    "  tf_model = Sequential()\n",
    "  tf_model.add(Flatten())\n",
    "  tf_model.add(Dense(128, activation=\"relu\"))\n",
    "  tf_model.add(Dense(128, activation=\"relu\"))\n",
    "  tf_model.add(Dense(64, activation=\"relu\"))\n",
    "  tf_model.add(Dense(32, activation=\"relu\"))\n",
    "  tf_model.add(Dense(len(set(Y_train)), activation=\"softmax\"))\n",
    "  tf_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "  tf_model.fit(X_train, Y_train, batch_size=32, nb_epoch=1, verbose=1)\n",
    "  score, acc = tf_model.evaluate(X_test, Y_test, verbose=1, batch_size=32)\n",
    "  print(\"Score: %.2f\" % (score))\n",
    "  print(\"Validation Accuracy: %.2f\" % (acc))\n",
    "  return tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)) as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_train.append(list(model.infer_vector(tweet.split())))\n",
    "        Y_train.append(expansion)\n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_test.append(list(model.infer_vector(tweet.split())))\n",
    "        Y_test.append(expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enikhil12/ags/venv/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 52144 arrays: [array([[ 3.97711694e-01],\n       [-7.09975839e-01],\n       [ 1.76661938e-01],\n       [ 6.11979067e-01],\n       [-4.60130811e-01],\n       [-3.22341956e-02],\n       [ 1.20659769e+00],\n       [-4.668729...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dd6f51eb096e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c05ddd9e1f03>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ags/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ags/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ags/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 52144 arrays: [array([[ 3.97711694e-01],\n       [-7.09975839e-01],\n       [ 1.76661938e-01],\n       [ 6.11979067e-01],\n       [-4.60130811e-01],\n       [-3.22341956e-02],\n       [ 1.20659769e+00],\n       [-4.668729..."
     ]
    }
   ],
   "source": [
    "classifier(X_train, Y_train, X_test, Y_test).save(str('gg')+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
