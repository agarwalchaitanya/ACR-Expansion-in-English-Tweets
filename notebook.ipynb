{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acronym Expansion in English Tweets \n",
    "> Acronyms are present all across social media to express information that is repetitive and well known. But acronyms can be ambiguous because there can be many expansions of the same acronym. This project aims to disambiguate between multiple expansions of an acronym given some context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from acrlist import acr\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acronym List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brb\n",
      "['be right back', 'bathroom break']\n",
      "cc\n",
      "['carbon copy', 'i understand']\n",
      "dl\n",
      "['download', 'down low', 'doing laundry']\n",
      "eta\n",
      "['estimated time of arrival', 'edited to add']\n",
      "gf\n",
      "['girlfriend', 'gluten-free']\n",
      "gg\n",
      "['good game', 'good grief']\n",
      "gl\n",
      "['good luck', 'get lost']\n",
      "hoas\n",
      "['hold on a second', 'heck of a shot']\n",
      "hw\n",
      "['homework', 'hardware']\n",
      "ic\n",
      "['i see', 'in character']\n",
      "im\n",
      "['instant messenger', 'instant message']\n",
      "k\n",
      "['ok', 'kiss']\n",
      "lol\n",
      "['laughing out loud', 'league of legends', 'lots of love', 'little old lady']\n",
      "na\n",
      "['not available', 'not applicable']\n",
      "nc\n",
      "['no comment', 'nice call', 'not cool']\n",
      "nm\n",
      "['not much', 'nevermind']\n",
      "np\n",
      "['no problem', 'neopets']\n",
      "ot\n",
      "['off topic', 'other topic', 'overtime']\n",
      "pm\n",
      "['pm', 'private message']\n",
      "pos\n",
      "['parent over shoulder', 'piece of shit', 'power of suggestion']\n",
      "re\n",
      "['regarding', 'resident evil']\n",
      "rotfl\n",
      "['rolling on the floor laughing', \"rolling over freakin' laughing\"]\n",
      "smh\n",
      "['shaking my head', 'smash my head', 'scratching my head']\n",
      "sos\n",
      "['someone over shoulder', 'save our souls', 'same old stuff', 'someone special']\n",
      "tc\n",
      "['take care', \"that's cool\"]\n",
      "ur\n",
      "['your', 'you are']\n",
      "wb\n",
      "['write back', 'welcome back', 'way bored']\n",
      "y\n",
      "['why', 'yawning']\n"
     ]
    }
   ],
   "source": [
    "for acronym, expansions in acr.items():\n",
    "    print(acronym)\n",
    "    print(expansions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "preprocess() prepares the dataset by replacing the expansion with the acronym and labelling it with the expansion. \n",
    "\n",
    "create_tagged_document() yields a tagset compatible with gensim's doc2vec module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet, acronym, expansion):\n",
    "    tweet = json.loads(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.replace(expansion, acronym)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tweets = []\n",
    "train_data_acronyms = []\n",
    "test_data_tweets = []\n",
    "test_data_acronyms = []\n",
    "\n",
    "for expansion in acr['ur']:\n",
    "    with open(\"train_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        train_data_tweets.append(preprocess(tweet,'ur', expansion).split())\n",
    "        train_data_acronyms.append(expansion)\n",
    "        \n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        test_data_tweets.append(preprocess(tweet,'ur', expansion).split())\n",
    "        test_data_acronyms.append(expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_document(split_tweets, data_acronyms):\n",
    "  for i, tweet in enumerate(split_tweets):\n",
    "    yield gensim.models.doc2vec.TaggedDocument(words=tweet, tags=[data_acronyms[i]])\n",
    "    \n",
    "train_data = list(create_tagged_document(train_data_tweets, train_data_acronyms))\n",
    "test_data = list(create_tagged_document(test_data_tweets, test_data_acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['imagine', 'spending', 'ur', 'entire', 'life', 'working', 'to', 'move', 'a', 'stone', 'for', 'the', 'pyramids', 'just', 'for', 'people', 'in', 'the', 'future', 'to', 'give', 'the', 'credit', 'to', 'aliens'], ['your'])\n",
      "\n",
      "TaggedDocument(['@socialmedia2day:', \"instagram's\", \"'checkout'\", 'on-platform', 'shopping', 'tools', 'are', 'slowly', 'being', 'rolled', 'out', 'to', 'more', 'profiles', '-', 'how', 'will', 'it', 'change', 'yoâ€¦'], ['your'])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])\n",
    "print()\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=50)\n",
    "dmodel.build_vocab(train_data)\n",
    "dmodel.train(train_data, total_examples=dmodel.corpus_count, epochs=dmodel.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer vector for a sample tweet:\n",
    "Now, that our Doc2Vec Model is trained, we can infer vectors for sample tweets. Here, we infer the vector for \"@realdonaldtrump ur the best!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = dmodel.infer_vector(\"@realdonaldtrump ur the best!\".split())\n",
    "res_tup = dmodel.docvecs.most_similar([v])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3730057   0.03873225  0.18107004  0.49245355  0.01146623  0.03212615\n",
      "  0.02637365 -0.26329508  0.09609044  0.00057228  0.00530779  0.04192339\n",
      "  0.09749373  0.108716   -0.06374781 -0.21909876  0.19668134  0.36210972\n",
      "  0.04201772  0.26893148 -0.39092022  0.29757932 -0.21173681 -0.05808842\n",
      "  0.5106051  -0.4477003   0.3142922   0.10621338  0.16630854  0.07746315\n",
      "  0.15002309  0.37434423  0.28614601  0.08227687 -0.2194723   0.11819626\n",
      "  0.10542516  0.36624688 -0.13511929 -0.00911398  0.0438503   0.00173114\n",
      " -0.10187595 -0.18324928 -0.14399385  0.36478555 -0.24429378  0.05013009\n",
      "  0.11764859  0.04702626]\n"
     ]
    }
   ],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Cosine Similarity Score\n",
    "Cosine Similarity shows intimacy between two vectors in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you are', 0.42928239703178406)\n"
     ]
    }
   ],
   "source": [
    "print(res_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1517795   0.59988225  0.17350073  1.4895848   0.6639548  -0.49600956\n",
      "  0.4742393  -1.0350697   0.595307    0.1914485  -0.32374594  0.1414308\n",
      " -0.49800748 -0.5194397   0.5195279  -1.3431836  -0.08917648 -0.09984987\n",
      "  0.02807552  0.9183444   0.7245053   0.18541253 -0.9700345  -0.1266044\n",
      "  0.66724527  0.24285768  1.0017112  -0.21868709  0.6122592  -0.5060775\n",
      "  0.08751622 -0.13614912  0.6777088   0.8275376  -1.668751    0.37461823\n",
      " -0.32881874  0.63455224 -0.83424807 -0.5471584  -1.0064769  -0.411073\n",
      " -0.81983805 -0.69994146 -0.22027121 -0.62154084 -0.04591395 -0.04753913\n",
      " -1.0543858  -0.1840892 ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "CATEGORIES = ['you are', 'your']\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(CATEGORIES.index(doc.tags[0]), np.array(model.infer_vector(doc.words))) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "Y_train, X_train = vector_for_learning(dmodel,train_data)\n",
    "Y_test, X_test = vector_for_learning(dmodel,test_data)\n",
    "print(X_train[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) for multi-class softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3742 - acc: 0.8331\n",
      "Epoch 2/50\n",
      "366555/366555 [==============================] - 13s 34us/step - loss: 0.3553 - acc: 0.8440\n",
      "Epoch 3/50\n",
      "366555/366555 [==============================] - 12s 32us/step - loss: 0.3513 - acc: 0.8456\n",
      "Epoch 4/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3500 - acc: 0.8467\n",
      "Epoch 5/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3484 - acc: 0.8471\n",
      "Epoch 6/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3474 - acc: 0.8476\n",
      "Epoch 7/50\n",
      "366555/366555 [==============================] - 14s 39us/step - loss: 0.3462 - acc: 0.8476\n",
      "Epoch 8/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3452 - acc: 0.8482\n",
      "Epoch 9/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3449 - acc: 0.8479\n",
      "Epoch 10/50\n",
      "366555/366555 [==============================] - 14s 39us/step - loss: 0.3445 - acc: 0.8484\n",
      "Epoch 11/50\n",
      "366555/366555 [==============================] - 11s 31us/step - loss: 0.3440 - acc: 0.8487\n",
      "Epoch 12/50\n",
      "366555/366555 [==============================] - 11s 31us/step - loss: 0.3436 - acc: 0.8486\n",
      "Epoch 13/50\n",
      "366555/366555 [==============================] - 15s 40us/step - loss: 0.3433 - acc: 0.8483\n",
      "Epoch 14/50\n",
      "366555/366555 [==============================] - 12s 32us/step - loss: 0.3431 - acc: 0.8492\n",
      "Epoch 15/50\n",
      "366555/366555 [==============================] - 15s 42us/step - loss: 0.3424 - acc: 0.8490\n",
      "Epoch 16/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3424 - acc: 0.8490\n",
      "Epoch 17/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3417 - acc: 0.8494\n",
      "Epoch 18/50\n",
      "366555/366555 [==============================] - 13s 36us/step - loss: 0.3416 - acc: 0.8489\n",
      "Epoch 19/50\n",
      "366555/366555 [==============================] - 16s 43us/step - loss: 0.3420 - acc: 0.8496\n",
      "Epoch 20/50\n",
      "366555/366555 [==============================] - 12s 34us/step - loss: 0.3416 - acc: 0.8498\n",
      "Epoch 21/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3413 - acc: 0.8499\n",
      "Epoch 22/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3412 - acc: 0.8496\n",
      "Epoch 23/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3413 - acc: 0.8500\n",
      "Epoch 24/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3409 - acc: 0.8500\n",
      "Epoch 25/50\n",
      "366555/366555 [==============================] - 13s 36us/step - loss: 0.3414 - acc: 0.8500\n",
      "Epoch 26/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3410 - acc: 0.8499\n",
      "Epoch 27/50\n",
      "366555/366555 [==============================] - 12s 34us/step - loss: 0.3409 - acc: 0.8504\n",
      "Epoch 28/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3404 - acc: 0.8507\n",
      "Epoch 29/50\n",
      "366555/366555 [==============================] - 12s 34us/step - loss: 0.3406 - acc: 0.8505\n",
      "Epoch 30/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3402 - acc: 0.8502\n",
      "Epoch 31/50\n",
      "366555/366555 [==============================] - 12s 33us/step - loss: 0.3406 - acc: 0.8505\n",
      "Epoch 32/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3407 - acc: 0.8500\n",
      "Epoch 33/50\n",
      "366555/366555 [==============================] - 14s 39us/step - loss: 0.3406 - acc: 0.8501\n",
      "Epoch 34/50\n",
      "366555/366555 [==============================] - 14s 37us/step - loss: 0.3407 - acc: 0.8502\n",
      "Epoch 35/50\n",
      "366555/366555 [==============================] - 13s 36us/step - loss: 0.3406 - acc: 0.8501\n",
      "Epoch 36/50\n",
      "366555/366555 [==============================] - 15s 40us/step - loss: 0.3406 - acc: 0.8496\n",
      "Epoch 37/50\n",
      "366555/366555 [==============================] - 13s 37us/step - loss: 0.3407 - acc: 0.8499\n",
      "Epoch 38/50\n",
      "366555/366555 [==============================] - 15s 42us/step - loss: 0.3411 - acc: 0.8502\n",
      "Epoch 39/50\n",
      "366555/366555 [==============================] - 16s 44us/step - loss: 0.3407 - acc: 0.8500\n",
      "Epoch 40/50\n",
      "366555/366555 [==============================] - 14s 38us/step - loss: 0.3402 - acc: 0.8504\n",
      "Epoch 41/50\n",
      "366555/366555 [==============================] - 14s 38us/step - loss: 0.3413 - acc: 0.8500\n",
      "Epoch 42/50\n",
      "366555/366555 [==============================] - 14s 38us/step - loss: 0.3414 - acc: 0.8508\n",
      "Epoch 43/50\n",
      "366555/366555 [==============================] - 15s 40us/step - loss: 0.3414 - acc: 0.8498\n",
      "Epoch 44/50\n",
      "366555/366555 [==============================] - 12s 32us/step - loss: 0.3407 - acc: 0.8509\n",
      "Epoch 45/50\n",
      "366555/366555 [==============================] - 12s 32us/step - loss: 0.3417 - acc: 0.8502\n",
      "Epoch 46/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3416 - acc: 0.8502\n",
      "Epoch 47/50\n",
      "366555/366555 [==============================] - 12s 32us/step - loss: 0.3419 - acc: 0.8506\n",
      "Epoch 48/50\n",
      "366555/366555 [==============================] - 11s 31us/step - loss: 0.3416 - acc: 0.8499\n",
      "Epoch 49/50\n",
      "366555/366555 [==============================] - 13s 35us/step - loss: 0.3414 - acc: 0.8504\n",
      "Epoch 50/50\n",
      "366555/366555 [==============================] - 14s 39us/step - loss: 0.3425 - acc: 0.8505\n",
      "159051/159051 [==============================] - 2s 10us/step\n",
      "0.6908596870458882 0.695437312561996\n"
     ]
    }
   ],
   "source": [
    "xx = tf.keras.utils.normalize(X_train)\n",
    "xtest = tf.keras.utils.normalize(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(50,),activation=tf.nn.relu))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation=tf.nn.relu))\n",
    "model.add(Dense(2, activation=tf.nn.softmax))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(np.array(xx), np.array(Y_train), epochs=50)\n",
    "val_loss, val_acc = model.evaluate(np.array(xtest), np.array(Y_test))\n",
    "print(val_loss, val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
