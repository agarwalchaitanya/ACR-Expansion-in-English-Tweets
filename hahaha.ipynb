{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lol123/.local/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from acrlist import acr\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tweets = []\n",
    "train_data_acronyms = []\n",
    "test_data_tweets = []\n",
    "test_data_acronyms = []\n",
    "\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        train_data_tweets.append(tweet.split())\n",
    "        train_data_acronyms.append(expansion)\n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        test_data_tweets.append(tweet.split())\n",
    "        test_data_acronyms.append(expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['hahahaha', '@andrewrobertso5', ',', 'btw', \"you've\", 'played', 'a', 'gg', 'just', 'now', 'https://t.co/fiooaattit'], ['good game'])\n",
      "TaggedDocument(['gg', '@dame_lillard.', '#nbaplayoffs'], ['good game'])\n",
      "TaggedDocument(['@vinijrmadrid', 'yh', 'i', 'agree', 'frenkie', 'is', 'overrated.', 'however,', 'have', 'you', 'ever', 'seen', 'a', 'gg', 'from', 'varane', 'without', 'ramos', 'by', 'his', 'side?'], ['good game'])\n",
      "TaggedDocument(['been', 'playing', 'a', 'ton', 'of', 'mw3', 'recently', 'and', 'dang....', 'it', 'is', 'such', 'a', 'gg'], ['good game'])\n"
     ]
    }
   ],
   "source": [
    "def create_tagged_document(split_tweets, data_acronyms):\n",
    "  for i, tweet in enumerate(split_tweets):\n",
    "    yield gensim.models.doc2vec.TaggedDocument(words=tweet, tags=[data_acronyms[i]])\n",
    "    \n",
    "train_data = list(create_tagged_document(train_data_tweets, train_data_acronyms))\n",
    "test_data = list(create_tagged_document(test_data_tweets, test_data_acronyms))\n",
    "print(train_data[1])\n",
    "print(train_data[0])\n",
    "print(test_data[1])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=50)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Vector & Prediction:\n",
      "[-0.0892152   0.26211968 -0.02735389 -0.15572265  0.01929537 -0.19861983\n",
      "  0.26863602  0.17998381 -0.05489823  0.20054746  0.00367287 -0.01180489\n",
      "  0.23448227 -0.11229099 -0.17844062  0.17398682  0.06106514 -0.11505175\n",
      "  0.07442122  0.01000812  0.07821944  0.22736843  0.01226939 -0.06567281\n",
      "  0.1394131  -0.21332549  0.39639238  0.09560472 -0.14940207 -0.35117173\n",
      " -0.03404063  0.36091828  0.2219844   0.06580997  0.28710154  0.18908893\n",
      "  0.20198484  0.03063935 -0.02680985  0.18092312 -0.17294838 -0.15409076\n",
      "  0.10034756  0.11244082  0.13534042 -0.08144362  0.40700454  0.2515625\n",
      "  0.26704383 -0.22430523]\n",
      "('good game', 0.5791212916374207)\n",
      "good game\n",
      "['gg', '@dame_lillard.', '#nbaplayoffs']\n",
      "[-0.08902215  0.19461791  0.04666947 -0.09826236 -0.04084231 -0.16117726\n",
      " -0.01098028  0.14934994 -0.0390399   0.13313478  0.0411016  -0.00123444\n",
      "  0.16371612 -0.00248014 -0.22178026 -0.03816758  0.12654068 -0.23927973\n",
      "  0.10196987  0.13858032  0.06449875  0.03366532  0.19333926 -0.15868677\n",
      "  0.00103347 -0.18601365  0.42940652  0.09423934  0.1371904  -0.08107458\n",
      "  0.10601971 -0.05460167  0.02749295  0.11939387  0.11402821  0.21265748\n",
      "  0.23256356 -0.0747572  -0.15551946  0.16040257  0.00792108 -0.07104084\n",
      " -0.08676604 -0.10438718  0.10499121 -0.02685267  0.23305218  0.00150446\n",
      " -0.03597737  0.04089317]\n",
      "[-0.03066132  0.07338227 -0.04024751  0.024893   -0.03844139 -0.08541031\n",
      " -0.00308233  0.03418827 -0.01459765  0.062407    0.04310888 -0.00813378\n",
      "  0.11070788 -0.00489493 -0.04674684  0.02998505  0.03796778 -0.07158925\n",
      "  0.03103952  0.07547396 -0.014447    0.016976    0.06677223 -0.05298034\n",
      " -0.01915596 -0.09691783  0.11439505  0.02572766  0.08791    -0.01076654\n",
      "  0.06091332 -0.05655839 -0.01911412  0.01416167  0.04144509  0.04208394\n",
      "  0.07628425 -0.07037362 -0.07983842  0.10773148 -0.05417501 -0.05673566\n",
      " -0.04046277 -0.03336854  0.04366647 -0.04008534  0.15057269  0.02892179\n",
      "  0.02868136  0.07315812]\n"
     ]
    }
   ],
   "source": [
    "v = model.infer_vector(\"el classico was gg\".split())\n",
    "res_tup = model.docvecs.most_similar([v])[0]\n",
    "print(\"Doc2Vec Vector & Prediction:\")\n",
    "print(np.array(v))\n",
    "print(res_tup)\n",
    "print(train_data[0].tags[0])\n",
    "print(train_data[0].words)\n",
    "print(model.infer_vector(train_data[0].words))\n",
    "print(model.infer_vector(train_data[0].words, steps=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, Y_train, X_test, Y_test):\n",
    "  #takes Doc2Vec as input layer instead of Word Embeddings, and trains classifiers for each acronym\n",
    "  tf_model = Sequential()\n",
    "  tf_model.add(Flatten())\n",
    "  tf_model.add(Dense(128, activation=\"relu\", input_shape=(50,)))\n",
    "  tf_model.add(Dense(64, activation=\"relu\"))\n",
    "  tf_model.add(Dense(len(set(Y_train)), activation=\"softmax\"))\n",
    "  tf_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "  tf_model.fit(X_train, Y_train, batch_size=32, nb_epoch=3, verbose=1)\n",
    "  score, acc = tf_model.evaluate(X_test, Y_test, verbose=1, batch_size=32)\n",
    "  print(\"Score: %.2f\" % (score))\n",
    "  print(\"Validation Accuracy: %.2f\" % (acc))\n",
    "  return tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09955219  0.21355869  0.00499058 -0.03739279 -0.13394399 -0.15706407\n",
      "  0.06024845  0.09879696  0.04302968  0.08702505  0.01033139 -0.0045344\n",
      "  0.17512321 -0.01450355 -0.15737897  0.06691764  0.16409363 -0.12645192\n",
      "  0.06212486  0.14132154  0.00557158 -0.02015231  0.17861135 -0.22822525\n",
      " -0.00230958 -0.20046216  0.45161894  0.06304871  0.13707675 -0.09345617\n",
      "  0.10469544 -0.05117442  0.12072166  0.01753756  0.17489526  0.22259988\n",
      "  0.17537904 -0.06440052 -0.16848782  0.2161144   0.08175163 -0.09842881\n",
      " -0.10066889 -0.06417364  0.09179608 -0.01166982  0.262457   -0.03282921\n",
      " -0.04030594  0.11259526]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], np.array(model.infer_vector(doc.words))) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "'''\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_train.append(np.array(model.infer_vector(tweet.split())))\n",
    "        Y_train.append(expansion)\n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\"\"\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_test.append(np.array(model.infer_vector(tweet.split())))\n",
    "        Y_test.append(expansion)\n",
    "'''\n",
    "Y_train, X_train = vector_for_learning(model,train_data)\n",
    "Y_test, X_test = vector_for_learning(model,test_data)\n",
    "print(X_train[0])\n",
    "# Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_model = classifier(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lol123/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.7531451365449524\n",
      "Testing F1 score for movie plots: 0.7548659004307904\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy for movie plots%s' % accuracy_score(Y_test, y_pred))\n",
    "print('Testing F1 score for movie plots: {}'.format(f1_score(Y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
