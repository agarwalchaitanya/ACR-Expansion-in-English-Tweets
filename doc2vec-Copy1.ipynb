{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from acrlist import acr\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tweets = []\n",
    "train_data_acronyms = []\n",
    "test_data_tweets = []\n",
    "test_data_acronyms = []\n",
    "\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        train_data_tweets.append(tweet.split())\n",
    "        train_data_acronyms.append(expansion)\n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        test_data_tweets.append(tweet.split())\n",
    "        test_data_acronyms.append(expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['hahahaha', '@andrewrobertso5', ',', 'btw', \"you've\", 'played', 'a', 'gg', 'just', 'now', 'https://t.co/fiooaattit'], ['good game'])\n",
      "TaggedDocument(['gg', '@dame_lillard.', '#nbaplayoffs'], ['good game'])\n",
      "TaggedDocument(['@vinijrmadrid', 'yh', 'i', 'agree', 'frenkie', 'is', 'overrated.', 'however,', 'have', 'you', 'ever', 'seen', 'a', 'gg', 'from', 'varane', 'without', 'ramos', 'by', 'his', 'side?'], ['good game'])\n",
      "TaggedDocument(['been', 'playing', 'a', 'ton', 'of', 'mw3', 'recently', 'and', 'dang....', 'it', 'is', 'such', 'a', 'gg'], ['good game'])\n"
     ]
    }
   ],
   "source": [
    "def create_tagged_document(split_tweets, data_acronyms):\n",
    "  for i, tweet in enumerate(split_tweets):\n",
    "    yield gensim.models.doc2vec.TaggedDocument(words=tweet, tags=[data_acronyms[i]])\n",
    "    \n",
    "train_data = list(create_tagged_document(train_data_tweets, train_data_acronyms))\n",
    "test_data = list(create_tagged_document(test_data_tweets, test_data_acronyms))\n",
    "print(train_data[1])\n",
    "print(train_data[0])\n",
    "print(test_data[1])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=50)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Vector & Prediction:\n",
      "[ 0.11610815 -0.09637776  0.13905996  0.02995563 -0.07188453  0.2620153\n",
      " -0.30154127 -0.0505563  -0.00487955 -0.22807415  0.03706959 -0.01216694\n",
      "  0.01565087  0.29274997  0.00140057  0.1917762   0.01242988 -0.04357467\n",
      "  0.24994367  0.02817337 -0.28806245  0.05678324  0.03414504 -0.01749791\n",
      " -0.31386104  0.08919828 -0.01148174  0.13156617  0.29845026  0.3045436\n",
      " -0.12027419 -0.4585482   0.04902999  0.3655776  -0.06130926 -0.07447112\n",
      " -0.3835978   0.36913866 -0.24127743  0.13647544  0.04905547  0.14781152\n",
      "  0.35666052 -0.04103333  0.07582591 -0.05212718 -0.05658853  0.01944257\n",
      "  0.15880005 -0.20482947]\n",
      "('good game', 0.5393985509872437)\n",
      "good game\n",
      "['gg', '@dame_lillard.', '#nbaplayoffs']\n",
      "[-0.12985063 -0.04627874  0.03161131 -0.08467706 -0.2287561   0.06795514\n",
      " -0.07820859 -0.01356712 -0.05916797  0.01745098  0.05414018 -0.08329126\n",
      "  0.00305788  0.06248035 -0.2394832   0.19107407  0.16665997  0.00084008\n",
      "  0.12116086  0.17829508 -0.11109615  0.06028922 -0.03060065  0.08611494\n",
      " -0.3200077   0.05053486  0.13236816  0.05662036 -0.04539014  0.11745931\n",
      " -0.1746185  -0.14443354  0.02044366  0.14361168  0.07836218 -0.08904336\n",
      " -0.17860562  0.18569832 -0.08841322  0.10748694  0.13415873 -0.05916728\n",
      "  0.15579452 -0.12586272 -0.0241234   0.10315344 -0.27855486  0.00764765\n",
      "  0.01585397 -0.24267147]\n",
      "[-0.10793194  0.00466497  0.01847992  0.01086709 -0.13601021  0.03255606\n",
      " -0.05974911 -0.00950711 -0.01357185 -0.01131652 -0.01993781 -0.04634437\n",
      "  0.00868908  0.03818129 -0.14849971  0.03071084  0.09987772  0.00558104\n",
      " -0.01372645  0.07172853 -0.07409219  0.06159503 -0.04153006  0.02682403\n",
      " -0.11234403  0.0302043   0.09379926  0.04672755  0.02012372  0.01027888\n",
      " -0.02544552 -0.03512321 -0.0006447   0.06737371  0.02214819 -0.05812532\n",
      " -0.09125892  0.07638759 -0.01134048  0.09429418  0.05388756 -0.04799015\n",
      "  0.0591087  -0.00766174 -0.023222    0.02723704 -0.12610655 -0.01114012\n",
      "  0.01870665 -0.11471359]\n"
     ]
    }
   ],
   "source": [
    "v = model.infer_vector(\"el classico was gg\".split())\n",
    "res_tup = model.docvecs.most_similar([v])[0]\n",
    "print(\"Doc2Vec Vector & Prediction:\")\n",
    "print(np.array(v))\n",
    "print(res_tup)\n",
    "print(train_data[0].tags[0])\n",
    "print(train_data[0].words)\n",
    "print(model.infer_vector(train_data[0].words))\n",
    "print(model.infer_vector(train_data[0].words, steps=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, Y_train, X_test, Y_test):\n",
    "  #takes Doc2Vec as input layer instead of Word Embeddings, and trains classifiers for each acronym\n",
    "  tf_model = Sequential()\n",
    "  tf_model.add(Flatten())\n",
    "  tf_model.add(Dense(128, activation=\"relu\", input_shape=(50,)))\n",
    "  tf_model.add(Dense(64, activation=\"relu\"))\n",
    "  tf_model.add(Dense(len(set(Y_train)), activation=\"softmax\"))\n",
    "  tf_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "  tf_model.fit(X_train, Y_train, batch_size=32, nb_epoch=3, verbose=1)\n",
    "  score, acc = tf_model.evaluate(X_test, Y_test, verbose=1, batch_size=32)\n",
    "  print(\"Score: %.2f\" % (score))\n",
    "  print(\"Validation Accuracy: %.2f\" % (acc))\n",
    "  return tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21349783  0.00385499 -0.0155456  -0.09196007 -0.27987805  0.04231379\n",
      " -0.07122794  0.00120678 -0.03665324  0.00331702  0.09748079 -0.02718358\n",
      "  0.00217732  0.11261013 -0.22843425  0.10728273  0.12206127  0.03555213\n",
      "  0.05433202  0.16443211 -0.07678624  0.14111248  0.02860417  0.04649205\n",
      " -0.33513197  0.08422614  0.08300844  0.04795912 -0.05511283  0.11051835\n",
      " -0.10859407 -0.19732864  0.02614488  0.17108893  0.10243626 -0.14008847\n",
      " -0.2484317   0.09366611 -0.06665147  0.15167387  0.13106623  0.01957054\n",
      "  0.16814987 -0.07924499  0.03290019  0.03467407 -0.34103623 -0.01297903\n",
      "  0.035485   -0.24149689]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], np.array(model.infer_vector(doc.words))) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "'''\n",
    "for expansion in acr['gg']:\n",
    "    with open(\"train_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_train.append(np.array(model.infer_vector(tweet.split())))\n",
    "        Y_train.append(expansion)\n",
    "    with open(\"test_data/\"+str(expansion)+\".txt\") as file:\n",
    "      tweets = file.readlines()\n",
    "      for tweet in tweets:\"\"\n",
    "        tweet = json.loads(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace(expansion, 'gg')\n",
    "        X_test.append(np.array(model.infer_vector(tweet.split())))\n",
    "        Y_test.append(expansion)\n",
    "'''\n",
    "Y_train, X_train = vector_for_learning(model,train_data)\n",
    "Y_test, X_test = vector_for_learning(model,test_data)\n",
    "print(X_train[0])\n",
    "# Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lol123/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.7517643448910709\n",
      "Testing F1 score for movie plots: 0.7537539921709248\n"
     ]
    }
   ],
   "source": [
    "#class_model = classifier(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lol123/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.7517643448910709\n",
      "Testing F1 score for movie plots: 0.7537539921709248\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy for movie plots%s' % accuracy_score(Y_test, y_pred))\n",
    "print('Testing F1 score for movie plots: {}'.format(f1_score(Y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
